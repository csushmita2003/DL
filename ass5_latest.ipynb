{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python310\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\python310\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python310\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python310\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Sample text for training the CBOW model (multiple sentences)\n",
    "text = \"\"\"Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "It supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
    "One of Python's significant advantages is its vast standard library, which provides modules and functions for a wide range of tasks, \n",
    "from web development to data analysis. Due to powerful libraries like NumPy and pandas, Python is widely used in scientific computing,\n",
    "artificial intelligence, machine learning, and data visualization. The language is designed with intuitive syntax, making it an excellent choice \n",
    "for both beginners and experienced developers. Python also boasts a strong community, contributing to a rich ecosystem of third-party packages \n",
    "available through the Python Package Index (PyPI). Furthermore, Python is cross-platform, allowing it to run on various operating systems \n",
    "such as Windows, macOS, and Linux without requiring significant changes to the code.\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data with context and target word\n",
    "training_data = []\n",
    "context_size = 2 # Adjust the context size as needed\n",
    "\n",
    "for i in range(context_size, len(tokens) - context_size):\n",
    "    context = tokens[i - context_size : i] + tokens[i + 1 : i + 1 + context_size]\n",
    "    target = tokens[i]\n",
    "    training_data.append(context + [target])  # Combine context and target into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "{'python': 0, 'programming': 1, 'language': 2, 'significant': 3, 'data': 4, 'analysis': 5, 'wide': 6, 'range': 7, 'tasks': 8, 'web': 9, 'development': 10, 'powerful': 11, 'due': 12, 'modules': 13, 'libraries': 14, 'like': 15, 'numpy': 16, 'pandas': 17, 'widely': 18, 'used': 19, 'functions': 20, 'library': 21, 'provides': 22, 'paradigms': 23, 'interpreted': 24, 'known': 25, 'simplicity': 26, 'readability': 27, 'supports': 28, 'multiple': 29, 'including': 30, 'changes': 31, 'procedural': 32, 'functional': 33, 'one': 34, 'advantages': 35, 'vast': 36, 'standard': 37, 'scientific': 38, 'code': 39, 'artificial': 40, 'run': 41, 'packages': 42, 'available': 43, 'package': 44, 'index': 45, 'pypi': 46, 'furthermore': 47, 'allowing': 48, 'various': 49, 'intelligence': 50, 'operating': 51, 'systems': 52, 'windows': 53, 'macos': 54, 'linux': 55, 'without': 56, 'requiring': 57, 'ecosystem': 58, 'rich': 59, 'contributing': 60, 'community': 61, 'machine': 62, 'learning': 63, 'visualization': 64, 'designed': 65, 'intuitive': 66, 'syntax': 67, 'making': 68, 'excellent': 69, 'choice': 70, 'beginners': 71, 'experienced': 72, 'developers': 73, 'also': 74, 'boasts': 75, 'strong': 76, 'computing': 77}\n"
     ]
    }
   ],
   "source": [
    "# Define the CBOW model using Word2Vec\n",
    "model_cbow = Word2Vec(sentences=[tokens], vector_size=500, window=context_size, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(model_cbow.wv.key_to_index)  # For Gensim 4.0+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBOW model\n",
    "model_cbow.train(training_data, total_examples=len(training_data), epochs=10)\n",
    "\n",
    "# Sample test data with known vocabulary\n",
    "test_data = [\n",
    "    ([\"python\", \"language\"], \"linux\"),  # Context: python, language → Target: syntax\n",
    "    ([\"widely\", \"used\"], \"artifical\"),   # Context: widely, used → Target: developers\n",
    "    ([\"data\", \"analysis\"], \"pandas\"),      # Context: data, analysis → Target: pandas\n",
    "    ([\"web\", \"development\"], \"django\"),    # Context: web, development → Target: django\n",
    "    ([\"intuitive\", \"syntax\"], \"readability\"),  # Context: intuitive, syntax → Target: readability\n",
    "    ([\"machine\", \"learning\"], \"artificial\"),  # Context: machine, learning → Target: artificial\n",
    "    ([\"libraries\", \"like\"], \"numpy\"),       # Context: libraries, like → Target: numpy\n",
    "    ([\"programming\", \"languages\"], \"python\"),  # Context: programming, languages → Target: python\n",
    "    ([\"powerful\", \"libraries\"], \"numpy\"),    # Context: powerful, libraries → Target: numpy\n",
    "    ([\"strong\", \"community\"], \"supporting\")  # Context: strong, community → Target: supporting\n",
    "]\n",
    "\n",
    "# Variables to calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['python', 'language'], Predicted: linux, Actual: linux\n",
      "Context: ['widely', 'used'], Predicted: artificial, Actual: artifical\n",
      "Context: ['data', 'analysis'], Predicted: choice, Actual: pandas\n",
      "Context: ['web', 'development'], Predicted: designed, Actual: django\n",
      "Context: ['intuitive', 'syntax'], Predicted: advantages, Actual: readability\n",
      "Context: ['machine', 'learning'], Predicted: artificial, Actual: artificial\n",
      "Context: ['libraries', 'like'], Predicted: powerful, Actual: numpy\n",
      "One or more context words not in vocabulary: ['programming', 'languages']\n",
      "Context: ['powerful', 'libraries'], Predicted: numpy, Actual: numpy\n",
      "Context: ['strong', 'community'], Predicted: various, Actual: supporting\n"
     ]
    }
   ],
   "source": [
    "# Predict and compare\n",
    "for context, actual_target in test_data:\n",
    "    try:\n",
    "        # Check if all words in context are present in the model's vocabulary\n",
    "        if all(word in model_cbow.wv.key_to_index for word in context):\n",
    "            predicted_word = model_cbow.wv.most_similar(positive=context, topn=2)\n",
    "            predicted_word_text = predicted_word[0][0]  # Get the most similar word\n",
    "            print(f\"Context: {context}, Predicted: {predicted_word_text}, Actual: {actual_target}\")\n",
    "            if predicted_word_text == actual_target:\n",
    "                correct_predictions += 1\n",
    "        else:\n",
    "            print(f\"One or more context words not in vocabulary: {context}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - Context: {context}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CBOW model: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"Accuracy of the CBOW model: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
